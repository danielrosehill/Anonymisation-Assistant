# LLM Assistant Config: Documentation Anonymization Assistant (Whistleblower Assistant)

*17-Dec-24*

[![View on Hugging Face](https://img.shields.io/badge/View%20on-Hugging%20Face-ff9b34?style=for-the-badge&logo=huggingface&logoColor=white)](https://hf.co/chat/assistant/6760b39606189c9ac0656dd6)

![alt text](images/header.png)

The purpose of this repository and the sharing of this configuration is to serve the public good in a very small way by providing one or possibly more model configurations for large language model assistants instructed to help the user with anonymizing documents for the purpose of safely sharing information without disclosing ones personal identity. 

To be more opaque: The use case envisioned here is helping whistleblowers.

While in general I try to consolidate my LLM assistant configurations in a single repository, occasionally I feel that the projects are potentially significant enough to warrant their own repositories.

## Notes About Context & Approach

When redacting information, whistleblowers commonly take the approach of trying to substitute identifiable or indirectly identifiable information with replacement data that alters particulars which are unimportant in the broad context. The intention in developing this assistant was to try to bring a little bit of automation to the process in order to make it as easy as possible.

As in any use case involving AI, human supervision is warranted and recommended. These configurations were written with the assumption that the user in question would likely wish to scrutinize carefully the modified output.

 The configuration Includes specifics about what type of personal information should always be redacted. The example provided in V-1 Is intended to guide the assistant in the style to be followed. It does require that the model leverage some inference in order to distinguish autonomously between information that *must* be redacted in all cases and information which must *never* be redacted in any case.  

Taking this configuration as a starting point, there are several variations on this theme which could also be helpful and useful. One example is that the Assistant could be configured to proposed substitutions for identifiable data before implementing them. This could prevent the situation in which these substitute synthetic data which the LLM decides upon is not sufficiently distinguished from the real facts. This configuration is intended more as a proof of concept and to demonstrate the use case than as a definitive text. But it's hoped that it will provide some utility for those requiring its services. .    

## Demonstration Documents

The test data folder provides examples of the `V1` configuration which I wrote by hand in action. 

The underlying model was `Llama 3.3 70B Instruct `as hosted and deployed on Hugging Face Chat.

These test runs demonstrated overall good adherence to the requirements as set down in the test prompt. The before and after texts can be viewed in the `test data` folder.  The diary entries which were used for the purpose of testing contain synthetic data generated by the same model. Any likeness to any individuals or organizations in the scenarios depicted was entirely unintended.   

